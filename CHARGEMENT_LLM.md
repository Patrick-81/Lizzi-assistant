# âœ… PRÃ‰-CHARGEMENT DU MODÃˆLE LLM

## ğŸ› ProblÃ¨me

**SymptÃ´me** : La reconnaissance vocale fonctionne, mais Lizzi ne rÃ©pond pas aux messages.

**Cause** : Le modÃ¨le LLM n'Ã©tait pas chargÃ© en mÃ©moire VRAM au dÃ©marrage du serveur. Ollama charge les modÃ¨les "Ã  la demande" (lazy loading), ce qui crÃ©ait un dÃ©lai ou timeout lors de la premiÃ¨re requÃªte.

## ğŸ”§ Solution AppliquÃ©e

### Fonction de Warm-up

Ajout d'une fonction `warmupModel()` qui :
1. CrÃ©e une instance de l'assistant
2. Envoie une requÃªte simple "Bonjour"
3. Force le chargement du modÃ¨le en VRAM
4. Met en cache l'instance pour les requÃªtes suivantes

### Code AjoutÃ© (src/server.ts)

```typescript
async function warmupModel() {
  try {
    console.log('ğŸ”¥ PrÃ©-chargement du modÃ¨le LLM...');
    const assistant = new Assistant();
    await assistant.initialize();
    
    // Envoie une requÃªte simple pour charger le modÃ¨le en mÃ©moire
    await assistant.chat('Bonjour');
    
    console.log('âœ… ModÃ¨le LLM chargÃ© en mÃ©moire');
    assistants.set('default', assistant);
  } catch (error) {
    console.error('âš ï¸  Erreur prÃ©-chargement modÃ¨le:', error);
  }
}
```

### Appel au DÃ©marrage

La fonction est appelÃ©e automatiquement au dÃ©marrage du serveur :

```typescript
https.createServer(httpsOptions, app).listen(PORT, async () => {
  console.log(`ğŸ”’ Serveur HTTPS dÃ©marrÃ© sur https://localhost:${PORT}`);
  console.log(`ğŸ“¡ ConnectÃ© Ã  Ollama sur ${process.env.OLLAMA_HOST}`);
  console.log(`ğŸ¤– ModÃ¨le: ${process.env.MODEL_NAME}`);
  
  // PrÃ©-charge le modÃ¨le en arriÃ¨re-plan
  warmupModel().catch(console.error);
});
```

## ğŸ“Š RÃ©sultat

### Avant
```bash
curl http://orion:11434/api/ps
{
  "models": []  # Aucun modÃ¨le chargÃ©
}
```

### AprÃ¨s
```bash
curl http://orion:11434/api/ps
{
  "models": [
    {
      "name": "ministral-3b-Q4:latest",
      "size_vram": 5955967104,  # ~5.5 GB chargÃ© en VRAM
      "expires_at": "..."
    }
  ]
}
```

## ğŸš€ Logs au DÃ©marrage

```
ğŸ¤ Reconnaissance vocale initialisÃ©e
ğŸ”’ Serveur HTTPS dÃ©marrÃ© sur https://localhost:3001
ğŸ“¡ ConnectÃ© Ã  Ollama sur http://orion:11434
ğŸ¤– ModÃ¨le: ministral-3b-Q4:latest
ğŸ”¥ PrÃ©-chargement du modÃ¨le LLM...
â³ Vectorisation de 10 faits en cours...
âœ… MÃ©moire prÃªte : 10 faits, 10 vecteurs.
ğŸ” VÃ©rification mÃ©morisation pour: Bonjour
â­• Pas de mot-clÃ© de mÃ©morisation dÃ©tectÃ©
ğŸ” RequÃªte Ã©largie: Bonjour
ğŸ” Recherche vectorielle: 0/10 faits trouvÃ©s (seuil: 0.5)
ğŸ“Š Cache: 10/10 vecteurs (0 manquants)
ğŸ“š 0 faits pertinents trouvÃ©s
âœ… ModÃ¨le LLM chargÃ© en mÃ©moire
```

## ğŸ’¡ Avantages

### 1. RÃ©ponse InstantanÃ©e
- Pas d'attente au premier message
- ModÃ¨le dÃ©jÃ  prÃªt en VRAM
- ExpÃ©rience utilisateur fluide

### 2. Instance RÃ©utilisÃ©e
- L'assistant warmup est mis en cache
- RÃ©utilisÃ© pour les messages suivants
- Pas de rechargement entre les requÃªtes

### 3. Gestion des Erreurs
- Si le warmup Ã©choue, le serveur continue
- Log d'erreur pour diagnostiquer
- L'utilisateur peut quand mÃªme envoyer des messages

## ğŸ¯ Comportement

### SÃ©quence de DÃ©marrage

1. **Serveur dÃ©marre** (1-2 secondes)
2. **Warmup lancÃ© en arriÃ¨re-plan** (5-10 secondes)
   - Initialise l'assistant
   - Charge la mÃ©moire long terme
   - Vectorise les faits
   - Envoie requÃªte "Bonjour" Ã  Ollama
   - Ollama charge le modÃ¨le en VRAM
3. **ModÃ¨le prÃªt** âœ…

**Pendant le warmup** : Le serveur est dÃ©jÃ  accessible, mais la premiÃ¨re vraie requÃªte pourrait Ãªtre un peu lente si le warmup n'est pas terminÃ©.

**AprÃ¨s le warmup** : Toutes les requÃªtes sont rapides car le modÃ¨le est dÃ©jÃ  chargÃ©.

## ğŸ” VÃ©rification

### Commande de Test

```bash
# VÃ©rifier si le modÃ¨le est chargÃ©
curl -s http://orion:11434/api/ps | jq '.models[].name'

# Devrait afficher:
# "ministral-3b-Q4:latest"
# "all-minilm:latest"
```

### Dans les Logs

Chercher cette ligne :
```
âœ… ModÃ¨le LLM chargÃ© en mÃ©moire
```

Si elle apparaÃ®t â†’ Le modÃ¨le est prÃªt !

## ğŸ“ Notes Techniques

### VRAM UtilisÃ©e
- **ModÃ¨le principal** : ~5.5 GB (ministral-3b-Q4)
- **ModÃ¨le embeddings** : ~76 MB (all-minilm)
- **Total** : ~5.6 GB

### Expiration
Le modÃ¨le reste en mÃ©moire **5 minutes** aprÃ¨s la derniÃ¨re utilisation. Si aucune requÃªte n'est envoyÃ©e pendant ce dÃ©lai, Ollama le dÃ©charge automatiquement.

### Rechargement Automatique
Si le modÃ¨le est dÃ©chargÃ©, la prochaine requÃªte le rechargera (avec un lÃ©ger dÃ©lai). Le warmup garantit juste qu'il est prÃªt au dÃ©marrage.

## ğŸš€ Test Final

```bash
# 1. Le serveur est dÃ©jÃ  dÃ©marrÃ© avec le warmup

# 2. Ouvre l'interface
https://localhost:3001

# 3. Teste la reconnaissance vocale + rÃ©ponse
- Maintiens la barre d'espace
- Dis "Bonjour Lizzi"
- RelÃ¢che
- â†’ Lizzi devrait rÃ©pondre instantanÃ©ment !
```

---

**Statut** : âœ… ModÃ¨le LLM prÃ©-chargÃ© au dÃ©marrage

**BÃ©nÃ©fice** : RÃ©ponses instantanÃ©es dÃ¨s le premier message
